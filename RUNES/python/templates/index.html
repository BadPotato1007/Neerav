<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Live Whisper Transcription</title>
    <style>
        body { display: flex; flex-direction: column; align-items: center; justify-content: center; height: 100vh; background: #111; color: #fff; }
        #mic-container { position: relative; width: 200px; height: 200px; margin-bottom: 32px; }
        #mic-ring { position: absolute; left: 0; top: 0; width: 100%; height: 100%; border-radius: 50%; border: 8px solid #1db954; transition: box-shadow 0.1s; }
        #mic-icon { position: absolute; left: 50%; top: 50%; transform: translate(-50%,-50%); font-size: 100px; color: #1db954; cursor: pointer; }
        #transcript { width: 90vw; max-width: 600px; min-height: 100px; background: #222; padding: 1em; border-radius: 12px; font-size: 1.3em; }
    </style>
</head>

<body>
    <div id="mic-container">
        <div id="mic-ring"></div>
        <span id="mic-icon">&#127908;</span>
    </div>
    <div id="transcript">Press mic and speak...</div>
    <script>
        let isRecording = false;
        let mediaRecorder;
        let audioChunks = [];
        let audioContext, analyser, dataArray, source;
        const micIcon = document.getElementById('mic-icon');
        const ring = document.getElementById('mic-ring');
        const transcriptDiv = document.getElementById('transcript');

        async function startRecording() {
            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
            audioContext = new (window.AudioContext || window.webkitAudioContext)();
            analyser = audioContext.createAnalyser();
            source = audioContext.createMediaStreamSource(stream);
            source.connect(analyser);
            dataArray = new Uint8Array(analyser.fftSize);
            mediaRecorder = new MediaRecorder(stream, { mimeType: 'audio/webm' });
            audioChunks = [];
            mediaRecorder.ondataavailable = e => {
                if (e.data.size > 0) {
                    audioChunks.push(e.data);
                }
            };
            mediaRecorder.onstop = async () => {
                const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
                // send audio to backend
                transcriptDiv.textContent = "Transcribing...";
                const formData = new FormData();
                formData.append("file", audioBlob, "audio.webm");
                const resp = await fetch("http://127.0.0.1:8000/transcribe", { method: "POST", body: formData });
                const json = await resp.json();
                transcriptDiv.textContent = json.text || "[No speech detected]";
            };
            mediaRecorder.start();
            isRecording = true;
            visualize();
        }

        function stopRecording() {
            if (mediaRecorder && mediaRecorder.state !== "inactive") {
                mediaRecorder.stop();
            }
            if (audioContext) {
                audioContext.close();
            }
            isRecording = false;
        }

        function visualize() {
            if (!isRecording) return;
            analyser.getByteTimeDomainData(dataArray);
            let sum = 0;
            for (let i = 0; i < dataArray.length; i++) {
                const val = (dataArray[i] - 128) / 128;
                sum += val * val;
            }
            const rms = Math.sqrt(sum / dataArray.length);
            const scale = 1 + rms * 2.5;
            ring.style.boxShadow = `0 0 ${60 + rms*100}px 10px #1db95455`;
            ring.style.transform = `scale(${scale})`;
            requestAnimationFrame(visualize);
        }

        micIcon.onclick = () => {
            if (!isRecording) {
                startRecording();
                micIcon.style.color = "#fa5252";
            } else {
                stopRecording();
                micIcon.style.color = "#1db954";
            }
        };
    </script>
</body>
</html>











